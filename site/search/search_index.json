{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"mu-pipelines documentation \ud83d\ude80","text":"<p>Welcome to the mu-pipelines documentation! Mu-Pipelines is a configuration-driven data pipeline platform that enables easy data ingestion, transformation, and orchestration without complex coding.</p>"},{"location":"#features","title":"\ud83c\udf1f Features","text":"<ul> <li>Flexible Data Ingestion: Read from CSV, JSON, Parquet, Databases, and more.</li> <li>Powerful Transformations: SQL and Python-based processing.</li> <li>Seamless Data Integration: Supports Iceberg, Delta Lake, and more.</li> <li>Lightweight &amp; Scalable: Configuration-based, no heavy setup required.</li> </ul> <p>\ud83d\ude80 Why mu-pipelines? </p> <p>For CTOs &amp; Data Leaders:</p> <p>\u2705 Accelerates Data Delivery \u2013 Move data 10x faster by eliminating manual ops.</p> <p>\u2705 Reduces Engineering Load \u2013 Cut pipeline maintenance time by 40-60%, freeing engineers for higher-value work.</p> <p>\u2705 Minimizes Risk \u2013 Automated testing and validation prevent costly data errors before they hit production.</p> <p>For Data Engineers:</p> <p>\u2705 No More Pipeline Firefighting \u2013 Standardized CI/CD means fewer outages and less time fixing broken jobs.</p> <p>\u2705 Easier Scaling \u2013 Configuration-driven pipelines scale without writing new custom code for every integration.</p> <p>\ud83d\udcd6 Start by reading the Getting Started guide!</p> <p>Need to connect with our team or have requests for specific connectors please email us at mupipelines@gmail.com</p> <p>For more details please visit our website: https://mosaicsoft.wixsite.com/mu-pipelines</p>"},{"location":"connection_properties/","title":"Connection Properties Documentation","text":""},{"location":"connection_properties/#overview","title":"Overview","text":"<p>Connection properties are used to define the connection settings between the data pipeline and various external systems (e.g., databases, file storage, message queues). These properties allow you to configure how your data pipeline interacts with the source and destination systems, ensuring that it can securely and efficiently read from and write to external resources.</p>"},{"location":"connection_properties/#connection-properties-structure","title":"Connection Properties Structure","text":"<p>Connection properties typically include the following key parameters:</p> Parameter Type Required Description <code>connection_name</code> String \u2705 Yes The unique name used to identify the connection configuration. <code>host</code> String \u2705 Yes The host URL or IP address of the external system (e.g., database or server). <code>port</code> Integer \u2705 Yes The port number used for the connection. <code>username</code> String \u2705 Yes The username required to authenticate to the external system. <code>password</code> String \u2705 Yes The password associated with the username for authentication. <code>database_name</code> String \u2705 Yes The name of the database or schema to connect to. <code>ssl_enabled</code> Boolean \u274c No Whether to use SSL encryption for the connection (default: <code>false</code>). <code>connection_timeout</code> Integer \u274c No The timeout value for establishing a connection (in seconds). <code>extra_options</code> Object \u274c No Additional options or parameters specific to the type of connection (e.g., SSL certificates)."},{"location":"connection_properties/#detailed-explanation-of-parameters","title":"Detailed Explanation of Parameters","text":"<ul> <li> <p><code>connection_name</code>: This parameter is used to uniquely identify a connection configuration. It allows you to reference the connection easily when defining sources or destinations in the pipeline configuration.</p> </li> <li> <p><code>host</code>: Specifies the host or IP address of the external system to which the pipeline will connect. This could be a database server, cloud storage endpoint, or a message queue system.</p> </li> <li> <p><code>port</code>: The port number used by the external system for communication. For example, databases typically use ports like <code>5432</code> for PostgreSQL or <code>3306</code> for MySQL. If no specific port is needed, the default port will be used based on the service type.</p> </li> <li> <p><code>username</code>: The username for authentication to the external system. This credential is used to securely access the database, file system, or other resource. It is important to keep this value secure.</p> </li> <li> <p><code>password</code>: The password corresponding to the username for authentication. It is important to securely store and encrypt this value in configuration files to prevent unauthorized access.</p> </li> <li> <p><code>database_name</code>: The name of the database or schema you want to connect to. This parameter is especially important when working with SQL databases or systems that have multiple schemas or databases.</p> </li> <li> <p><code>ssl_enabled</code>: A boolean value that specifies whether to use SSL (Secure Sockets Layer) encryption for the connection. This is important for securing data transmission between the pipeline and the external system. If set to <code>true</code>, SSL encryption will be used.</p> </li> <li> <p><code>connection_timeout</code>: Defines the timeout period (in seconds) for establishing the connection. If the connection cannot be established within this time, the attempt will be aborted. This parameter helps manage connection failures gracefully.</p> </li> <li> <p><code>extra_options</code>: An optional object to provide additional configuration settings specific to the connection. For instance, you might need to provide SSL certificates, connection pooling settings, or other system-specific parameters.</p> </li> </ul>"},{"location":"connection_properties/#example-connection-configuration","title":"Example Connection Configuration","text":"<p>Here\u2019s an example of how connection properties might look in a JSON configuration for connecting to a PostgreSQL database:</p> <pre><code>{\n  \"connection_properties\": {\n    \"connection_name\": \"postgres_connection\",\n    \"host\": \"db.example.com\",\n    \"port\": 5432,\n    \"username\": \"user123\",\n    \"password\": \"securepassword\",\n    \"database_name\": \"data_pipeline_db\",\n    \"ssl_enabled\": true,\n    \"connection_timeout\": 30,\n    \"extra_options\": {\n      \"ssl_cert_path\": \"/path/to/ssl/certificate\",\n      \"pool_size\": 10\n    }\n  }\n}\n\n</code></pre>"},{"location":"connection_properties/#conclusion","title":"Conclusion","text":"<p>The Connection Properties are essential for establishing secure and efficient connections between your data pipeline and external systems. By defining these properties, you can configure various connection settings such as authentication, encryption, timeouts, and more. This flexibility ensures your pipeline can seamlessly interact with different data sources and destinations, whether it's a database, file system, or message queue.</p> <p>Proper management of connection properties is key to ensuring smooth and reliable data flow across your pipeline. Security features like SSL encryption, as well as customizable timeouts and additional settings, allow for a secure and performant connection to external resources.</p> <p>Incorporating these connection properties within your pipeline configuration allows you to easily manage different environments, credentials, and connection settings, ultimately leading to a more robust and secure data integration process.</p>"},{"location":"faq/","title":"FAQ \u2753","text":""},{"location":"faq/#how-do-i-install-mu-pipelines","title":"How do I install Mu-Pipelines?","text":"<p>Run:</p> <pre><code>pip install 'mu-pipelines-driver[spark]'\n</code></pre>"},{"location":"faq/#can-i-build-my-own-connector","title":"Can I build my own connector ?","text":"<p>Yes, that functionality is available in pro license</p>"},{"location":"faq/#how-do-i-add-connectionspasswords-to-my-source-and-destination","title":"How do i add connections/passwords to my source and destination?","text":"<p>You can add them in connection_properties.json file in root folder. For details see connection_properties page </p>"},{"location":"faq/#what-is-the-role-for-global_configjson","title":"What is the role for global_config.json ?","text":"<p>There are ceratin global properties you can apply to your whole project. See global_properties.json page for detailed analysis </p>"},{"location":"faq/#what-is-the-role-of-library","title":"What is the role of library ?","text":"<p>In future we plan to support core python/pandas in addition to spark. As we release library can be switched</p>"},{"location":"getting-started/","title":"Welcome to mu-pipelines!","text":"<p>This guide will help you set up and run Mu-Pipelines.</p>"},{"location":"getting-started/#1-install-mu-pipelines","title":"1\ufe0f\u20e3 Install mu-pipelines","text":"<p>Ensure you have Python 3 installed, then run:</p> <pre><code>pip install 'mu-pipelines-driver[spark]'\n</code></pre>"},{"location":"getting-started/#2-create-config-files","title":"2\ufe0f\u20e3 Create Config Files","text":"<p>Our config files are divided into three sections: <code>ingest</code>, <code>transform</code> and <code>destination</code>.</p>"},{"location":"getting-started/#configuration-parameters","title":"Configuration Parameters:","text":"<ul> <li>Ingestxx: This is the source from where data needs to be ingested</li> <li>Transformxx: This is where you want to transform data</li> <li>Destinationxx: The destination where data should be stored.</li> </ul> <p>Coming Soon </p> <ul> <li>Executexx: Run any python execute commands. e.g. ExecuteZIP, ExecuteFileCopy, ExecuteFileEncrypt etc </li> <li>Validation: Ability to run data quality checks based on parameters defined</li> <li>Alerts: Configure where you want to send alerts to </li> <li>Schedule: Add a schedule for your job </li> <li>Dependency: List of config's that the job is dependant on </li> </ul> <p>Refer to the config documentation for a list of available <code>execute</code> and <code>destination</code> commands.</p>"},{"location":"getting-started/#3-run-a-sample-config-file","title":"3\ufe0f\u20e3 Run a Sample Config File","text":"<p>In your notebook or Python file, add the following code:</p> <pre><code>from mu_pipelines_driver.run_config import run_config\n\ndf = run_config(\n    [\n        {\n            \"execution\": [\n                {\n                    \"type\": \"CSVReadCommand\",\n                    \"file_location\": \"/home/iceberg/data/file/people.csv\",\n                    \"delimiter\": \",\",\n                    \"quotes\": \"\\\"\",\n                    \"additional_attributes\": [\n                        {\"key\": \"header\", \"value\": \"True\"}\n                    ]\n                }\n            ],\n            \"destination\": [\n                {\n                    \"type\": \"table\",\n                    \"table_name\": \"crm.raw.people\",\n                    \"mode\": \"overwrite\"\n                }\n            ]\n        }\n    ],\n    {\"library\": \"spark\"},\n    {\"connections\": []}\n)\n</code></pre>"},{"location":"getting-started/#4-run-config-using-spark-submit","title":"4\ufe0f\u20e3 Run Config Using Spark Submit","text":"<p>Run your configuration using Spark Submit.</p> <p>You can run a config in production spark by using spark submit </p> <p>Create a sample python script o import module </p> <pre><code>from mu_pipelines_driver.__main__ import main\n\nmain()\n\n</code></pre> <p>Spark submit command</p> <pre><code>spark-submit /home/scripts/app_args.py --global-properties /home/scripts/global-properties.json --connection-properties /home/scripts/connection-properties.json /home/scripts/raw/people/people.json\n\n</code></pre>"},{"location":"getting-started/#5-run-config-using-airflow","title":"5\ufe0f\u20e3 Run Config Using Airflow","text":"<p>Execute the config using Airflow for workflow orchestration. Use spark connector in airflow to create issue a spark submit command </p>"},{"location":"getting-started/#6-chain-different-configurations-to-meet-business-needs","title":"6\ufe0f\u20e3 Chain Different Configurations to Meet Business Needs","text":"<p>Mu-pipelines allows you to chain multiple configurations together to address complex business requirements.</p> <p>\ud83c\udfaf Want to add a custom connector? Stay tuned for developer guides!</p> <p>Need to connect with our team or have requests for specific connectors please email us at mupipelines@gmail.com</p> <p>For more details please visit our website: https://mosaicsoft.wixsite.com/mu-pipelines</p>"},{"location":"global_properties/","title":"Global Properties Documentation","text":""},{"location":"global_properties/#overview","title":"Overview","text":"<p>Global properties define configuration settings that apply across the entire data pipeline or Spark job execution. These properties typically control the environment, performance settings, logging configuration, and other global settings that can be shared across different commands in the pipeline. Configuring these properties helps ensure that your pipeline operates consistently and efficiently.</p>"},{"location":"global_properties/#global-properties-structure","title":"Global Properties Structure","text":"Parameter Type Required Description <code>spark_master</code> String \u2705 Yes The Spark master URL that defines the cluster to run Spark jobs on. <code>spark_app_name</code> String \u2705 Yes The name of the Spark application. <code>spark_executor_cores</code> Integer \u274c No Number of cores to use for each executor in the Spark job. <code>spark_executor_memory</code> String \u274c No Amount of memory to allocate for each Spark executor (e.g., \"4g\"). <code>spark_driver_memory</code> String \u274c No Amount of memory to allocate for the Spark driver (e.g., \"2g\"). <code>spark_sql.shuffle_partitions</code> Integer \u274c No Number of partitions to use when shuffling data in Spark SQL. <code>spark_event_log_dir</code> String \u274c No Directory to store the event logs for Spark jobs. <code>spark_conf</code> Object \u274c No Additional Spark configurations (e.g., custom settings for the Spark cluster)."},{"location":"global_properties/#detailed-explanation-of-parameters","title":"Detailed Explanation of Parameters","text":"<ul> <li><code>spark_master</code>: Specifies the master URL for the Spark cluster. This defines where the Spark job should run. The value could be:</li> <li><code>\"local\"</code> for running Spark locally.</li> <li>A URL for a cluster manager like <code>\"spark://&lt;host&gt;:&lt;port&gt;\"</code>.</li> <li> <p><code>\"yarn\"</code> or <code>\"mesos\"</code> for distributed execution on YARN or Mesos clusters.</p> </li> <li> <p><code>spark_app_name</code>: A string representing the name of the Spark application. This name will appear in the Spark UI and logs. It helps in identifying the application during job execution.</p> </li> <li> <p><code>spark_executor_cores</code>: Defines how many cores to allocate for each executor. Executors are the processes running on worker nodes that execute Spark tasks. The number of cores can affect the parallelism of your tasks.</p> </li> <li> <p><code>spark_executor_memory</code>: Specifies how much memory to allocate to each executor. For example, <code>\"4g\"</code> would allocate 4 GB of memory for each executor. The appropriate memory allocation depends on the size of the data and the tasks being executed.</p> </li> <li> <p><code>spark_driver_memory</code>: Specifies how much memory to allocate to the Spark driver. The driver is responsible for maintaining the Spark application\u2019s state, managing jobs, and scheduling tasks on executors. A higher memory allocation might be needed for large jobs.</p> </li> <li> <p><code>spark_sql.shuffle_partitions</code>: Defines the number of partitions to use when shuffling data in Spark SQL operations. A higher number of partitions can improve parallelism but may increase overhead.</p> </li> <li> <p><code>spark_event_log_dir</code>: Specifies the directory where Spark logs the events for the application. These logs are helpful for debugging and understanding the execution of the job.</p> </li> <li> <p><code>spark_conf</code>: This parameter allows you to pass additional Spark configurations that are specific to your environment or application. For example, you can set Spark settings like <code>spark.sql.autoBroadcastJoinThreshold</code> to control the size of data broadcasted in joins.</p> </li> </ul>"},{"location":"global_properties/#example-global-properties-for-spark","title":"Example Global Properties for Spark","text":"<p>Here\u2019s an example of a JSON configuration for Spark global properties:</p> <pre><code>{\n  \"global_properties\": {\n    \"spark_master\": \"spark://spark-cluster:7077\",\n    \"spark_app_name\": \"data_pipeline_app\",\n    \"spark_executor_cores\": 4,\n    \"spark_executor_memory\": \"8g\",\n    \"spark_driver_memory\": \"4g\",\n    \"spark_sql.shuffle_partitions\": 200,\n    \"spark_event_log_dir\": \"/tmp/spark-logs\",\n    \"spark_conf\": {\n      \"spark.sql.autoBroadcastJoinThreshold\": \"10485760\",\n      \"spark.sql.shuffle.partitions\": \"100\"\n    }\n  }\n}\n\n</code></pre>"},{"location":"destination-command/destinationCSV/","title":"DestinationCSV Command Documentation","text":"<p>The DestinationCSV command is used to specify the destination for writing data in CSV format. This can be useful when you need to store processed or ingested data into CSV files. Below is an example configuration for writing data to a CSV file.</p> <pre><code>{\n  \"destination\": [\n    {\n      \"type\": \"destinationCSV\",\n      \"file_location\": \"/home/data/output/processed_data.csv\",\n      \"delimiter\": \",\",\n      \"header\": true,\n      \"quotes\": \"\\\"\",\n      \"additional_attributes\": [\n        { \"key\": \"mode\", \"value\": \"overwrite\" }\n      ]\n    }\n  ]\n}\n\n</code></pre>"},{"location":"destination-command/destinationCSV/#destinationcsv-command-parameters","title":"DestinationCSV Command - Parameters","text":""},{"location":"destination-command/destinationCSV/#parameters","title":"Parameters","text":"Parameter Type Required Description <code>type</code> String \u2705 Yes The type of command (<code>destinationCSV</code>). <code>file_location</code> String \u2705 Yes The location of the output CSV file. This can be a relative or absolute path. <code>delimiter</code> String \u2705 Yes The delimiter used to separate values in the CSV file (e.g., <code>,</code>, <code>;</code>, <code>\\t</code>). <code>header</code> Boolean \u2705 Yes Whether to include a header row in the CSV file (<code>true</code> or <code>false</code>). <code>quotes</code> String \u2705 Yes The character used to quote strings in the CSV file (e.g., <code>\"</code>). <code>additional_attributes</code> Array \u274c No Additional attributes for customization (e.g., mode, append, etc.)."},{"location":"destination-command/destinationCSV/#detailed-explanation-of-parameters","title":"Detailed Explanation of Parameters","text":"<ul> <li> <p><code>type</code>: Always set to <code>\"destinationCSV\"</code>. This indicates that the destination is a CSV file.</p> </li> <li> <p><code>file_location</code>: The file path where the CSV data will be written. This can be either an absolute or a relative file path. For example, <code>/home/data/output/processed_data.csv</code>. The file will be created at the given location if it does not already exist.</p> </li> <li> <p><code>delimiter</code>: Specifies the character used to separate values in the CSV file. The default is a comma (<code>,</code>), but other characters such as semicolons (<code>;</code>) or tabs (<code>\\t</code>) can be used.</p> </li> <li> <p><code>header</code>: If set to <code>true</code>, the CSV file will include a header row containing the column names. If set to <code>false</code>, no header row will be included.</p> </li> <li> <p><code>quotes</code>: Defines the character used to quote string values in the CSV file. This is typically used to enclose values that contain special characters or delimiters. The default is double-quote (<code>\"</code>), but you can specify other characters like single-quote (<code>'</code>).</p> </li> <li> <p><code>additional_attributes</code>: This is an optional parameter. It allows for extra configuration. For example, you might use this to specify the <code>mode</code> of writing to the file. Common modes are:</p> </li> <li><code>overwrite</code>: Replaces the existing file if it exists.</li> <li><code>append</code>: Adds new data to the end of the existing file.</li> </ul>"},{"location":"destination-command/destinationCSV/#conclusion","title":"Conclusion","text":"<p>The DestinationCSV command provides an easy way to write processed data into CSV files, with full control over the formatting, delimiters, headers, and other aspects of the CSV file. This configuration is particularly useful for creating human-readable, portable data files that can be easily shared or imported into other systems. It offers a simple interface for configuring output files, making it suitable for use cases where CSV files need to be written as a part of a data pipeline.</p>"},{"location":"destination-command/destinationCSV/#related-commands","title":"Related Commands","text":"<ul> <li>IngestCSV: Reads data from a CSV file and uses it as input for your data pipeline. This command is helpful when you're looking to ingest data from CSV files for further processing.</li> </ul>"},{"location":"destination-command/destinationDefaultCatalog/","title":"Deafult Catalog (Data Lake/Lakehouse)","text":""},{"location":"destination-command/destinationDefaultCatalog/#default-catalog-as-destination-documentation","title":"Default Catalog as Destination Documentation \ud83d\udcca","text":"<p>The DestinationDefaultCatalog destination configuration allows you to define where the data will be written in the pipeline. The destination can specify an table where data will be loaded, and it supports different modes for managing data. It can be any hive metastore, spark managed catalog table, iceberg, delta. It uses default spark catalog to push data to. </p>"},{"location":"destination-command/destinationDefaultCatalog/#parameters","title":"Parameters","text":"Parameter Type Required Description <code>type</code> String \u2705 Yes The destination type (<code>DestinationDefaultCatalog</code> for Iceberg tables). <code>table_name</code> String \u2705 Yes The name of the Iceberg table where data will be written. <code>mode</code> String \u2705 Yes Defines how the data is written to the table. Possible values: <code>overwrite</code>, <code>append</code>, etc."},{"location":"destination-command/destinationDefaultCatalog/#mode-options","title":"Mode Options","text":"<ul> <li><code>overwrite</code>: Replaces the data in the specified table with the new data. Any existing records in the table will be deleted.</li> <li><code>append</code>: Adds the new data to the existing records in the table without modifying the existing data.</li> </ul>"},{"location":"destination-command/destinationDefaultCatalog/#configuring-spark-with-iceberg-as-the-default-catalog","title":"Configuring Spark with Iceberg as the Default Catalog","text":"<p>Please note similar steps can be applied for Hive, delta etc </p> <p>To ensure that Apache Spark can interact with Iceberg as the default catalog when using it as a destination, you must configure Spark with the appropriate catalog settings. By default, Spark uses Hive as its catalog. To set Iceberg as the default, follow these steps:</p> <ol> <li>Add Iceberg as a dependency:    You need to add the Iceberg Spark extension to your Spark setup. Ensure the correct version of Iceberg is available in your Spark environment. Add the following dependency to your Spark session configuration:</li> </ol> <p><code>bash    --packages org.apache.iceberg:iceberg-spark3-runtime:0.13.1</code></p> <p>This will include the necessary Iceberg libraries for Spark.</p> <ol> <li>Configure the Default Catalog:    To set Iceberg as the default catalog in Spark, update the Spark session configuration. You can set Iceberg as the default catalog by using the following settings:</li> </ol> <p><code>bash    spark.sql.catalog.spark_catalog org.apache.iceberg.spark.SparkSessionCatalog    spark.sql.catalog.spark_catalog.type hadoop    spark.sql.catalog.spark_catalog.warehouse /path/to/warehouse</code></p> <ul> <li><code>spark_catalog</code> refers to the catalog in use (in this case, Iceberg).</li> <li>The <code>warehouse</code> location should point to the directory where your Iceberg tables are stored.</li> </ul> <p>With this configuration, Spark will use Iceberg as the default catalog for reading and writing tables.</p> <ol> <li>Setting Iceberg Table for the Destination:    Once Iceberg is set as the default catalog, you can specify the destination table in your pipeline configuration, like so:</li> </ol> <p><code>json    {      \"destination\": [        {          \"type\": \"DestinationDefaultCatalog\",          \"table_name\": \"crm.raw.people\",          \"mode\": \"overwrite\"        }      ]    }</code></p> <p>Here, <code>crm.raw.people</code> refers to an Iceberg table that will be written to, and Spark will handle the interaction with the Iceberg catalog.</p>"},{"location":"destination-command/destinationDefaultCatalog/#example-use-case","title":"Example Use Case","text":"<p>Scenario: You want to write data to the <code>crm.raw.people</code> Iceberg table and overwrite any existing records.</p>"},{"location":"destination-command/destinationDefaultCatalog/#json-configuration","title":"JSON Configuration:","text":"<pre><code>{\n  \"destination\": [\n    {\n      \"type\": \"DestinationDefaultCatalog\",\n      \"table_name\": \"crm.raw.people\",\n      \"mode\": \"overwrite\"\n    }\n  ]\n}\n</code></pre>"},{"location":"destination-command/destinationDefaultCatalog/#behavior","title":"Behavior","text":"<ul> <li><code>table_name</code>: Specifies the Iceberg table where the data should be loaded.</li> <li><code>mode</code>: Controls how the data is written to the destination table. Choose <code>overwrite</code> to replace existing data or <code>append</code> to add new records to the existing data.</li> </ul>"},{"location":"destination-command/destinationDefaultCatalog/#related-commands","title":"Related Commands","text":"<ul> <li>TransformSQL: For transforming the data before writing it to the destination.</li> </ul>"},{"location":"destination-command/destinationDefaultCatalog/#conclusion","title":"Conclusion","text":"<p>The DestinationDefaultCatalog command provides a simple and effective way to write data into Default catalog tables with flexible write modes like <code>overwrite</code> and <code>append</code>. Configuring Spark with Iceberg as the default catalog makes it easier to work with Iceberg tables without needing to manually specify the catalog for each interaction. Once configured, use this command to manage your data pipeline outputs in a distributed, scalable table format.</p>"},{"location":"ingest-command/ingestCSV/","title":"CSV","text":""},{"location":"ingest-command/ingestCSV/#ingestcsv-documentation","title":"IngestCSV Documentation \ud83d\udcc4","text":"<p>The <code>IngestCSV</code> is used to read and load data from a CSV file into the pipeline. It provides configuration options like file location, delimiter, quote characters, and headers to accommodate different CSV formats.</p>"},{"location":"ingest-command/ingestCSV/#parameters","title":"Parameters","text":"Parameter Type Required Description <code>type</code> String \u2705 Yes Specifies the type of the command (<code>IngestCSV</code>). <code>file_location</code> String \u2705 Yes The path to the CSV file to be read. Can be absolute or relative. <code>delimiter</code> String \u274c No The character used to separate columns in the CSV (default is <code>,</code>). <code>quotes</code> String \u274c No The character used for quoting text in CSV (default is <code>\"</code>). <code>additional_attributes</code> Array \u274c No Optional attributes to specify additional settings. Each attribute consists of a <code>key</code> and <code>value</code> pair. For example, you can specify if the CSV has headers."},{"location":"ingest-command/ingestCSV/#additional-attributes-example","title":"Additional Attributes Example","text":"<p>The <code>additional_attributes</code> parameter allows users to specify extra properties for the CSV file:</p> <ul> <li>key: The name of the attribute.</li> <li>value: The value associated with the attribute.</li> </ul>"},{"location":"ingest-command/ingestCSV/#example-of-header-attribute","title":"Example of Header Attribute","text":"<pre><code>\"additional_attributes\": [\n  {\n    \"key\": \"header\",\n    \"value\": \"True\"\n  }\n]\n</code></pre> <p>This indicates that the first row in the CSV file should be treated as column headers.</p>"},{"location":"ingest-command/ingestCSV/#example-use-case","title":"Example Use Case","text":"<p>Scenario: You need to read a CSV file named <code>people.csv</code> that has headers and uses commas as delimiters.</p>"},{"location":"ingest-command/ingestCSV/#json-configuration","title":"JSON Configuration:","text":"<pre><code>{\n  \"execution\": [\n    {\n      \"type\": \"IngestCSV\",\n      \"file_location\": \"/home/iceberg/data/file/people.csv\",\n      \"delimiter\": \",\",\n      \"quotes\": \"\"\",\n      \"additional_attributes\": [\n        {\n          \"key\": \"header\",\n          \"value\": \"True\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"ingest-command/ingestCSV/#behavior","title":"Behavior","text":"<ul> <li><code>file_location</code>: Specifies the file to be read. Ensure the path is correct.</li> <li><code>delimiter</code>: By default, CSV files are comma-delimited, but this can be customized for other formats (e.g., semicolons).</li> <li><code>quotes</code>: Handles quoted values in CSV files (e.g., <code>\"John, Doe\"</code>).</li> <li><code>header</code>: If set to <code>True</code>, the first row is treated as column headers.</li> </ul>"},{"location":"ingest-command/ingestCSV/#related-commands","title":"Related Commands","text":"<ul> <li>DestinationCSV: For writing data back into a CSV file.</li> <li>TransformSQL: For transforming the data after reading it.</li> </ul>"},{"location":"ingest-command/ingestCSV/#conclusion","title":"Conclusion","text":"<p>The IngestCSV is a flexible and configurable way to read CSV files into your pipeline, allowing for custom delimiters, quotes, and handling of headers.</p>"},{"location":"ingest-command/ingestJDBC/","title":"IngestJDBC Command Documentation - Coming Soon","text":""},{"location":"ingest-command/ingestJDBC/#overview","title":"Overview","text":"<p>The IngestJDBC command is used to ingest data from a JDBC-compatible database into your pipeline. This command allows you to extract data from relational databases and bring it into your data pipeline. It supports incremental loading, dynamically fetching the last processed value from the source database. This improves performance by only processing new or modified data since the last execution.</p> <p>With the ability to define lower and upper bounds, the system can dynamically query the source database for the incremental values based on a specific column, ensuring only the relevant data is ingested.</p>"},{"location":"ingest-command/ingestJDBC/#ingestjdbc-command-structure","title":"IngestJDBC Command Structure","text":"Parameter Type Required Description <code>type</code> String \u2705 Yes The type of command, always set to <code>IngestJDBC</code>. <code>connection_name</code> String \u2705 Yes The name of the connection to the database (referenced from Connection Properties). <code>query</code> String \u2705 Yes The SQL query to retrieve data from the source database. <code>lower_bound_query</code> String \u274c No The SQL query used to get the lower bound value for incremental loading. <code>upper_bound_query</code> String \u274c No The SQL query used to get the upper bound value for incremental loading. <code>destination</code> Object \u2705 Yes The destination configuration where the ingested data will be written. <code>batch_size</code> Integer \u274c No The number of rows to fetch per batch during data ingestion."},{"location":"ingest-command/ingestJDBC/#detailed-explanation-of-parameters","title":"Detailed Explanation of Parameters","text":"<ul> <li> <p><code>type</code>: Always set to <code>\"IngestJDBC\"</code>. This identifies the command type as a JDBC ingestion command.</p> </li> <li> <p><code>connection_name</code>: Specifies the connection to be used for connecting to the source database. The connection name must correspond to a pre-defined connection in the Connection Properties section.</p> </li> <li> <p><code>query</code>: The SQL query used to retrieve data from the source database. It should include a condition based on the incremental_column if incremental loading is enabled.</p> </li> <li> <p><code>lower_bound_query</code>: An optional SQL query that dynamically fetches the lower bound (the starting point) for the incremental load. This value can be used to determine the beginning of the range of data to be fetched. For example, it could be the maximum value of the <code>incremental_column</code> from the previous ingestion.</p> </li> <li> <p><code>upper_bound_query</code>: An optional SQL query that dynamically fetches the upper bound (the ending point) for the incremental load. This value can be used to determine the end of the range for data to be fetched. This ensures that only data within this range will be ingested in the next execution.</p> </li> <li> <p><code>batch_size</code>: Defines the number of rows to fetch per batch from the source database. Fetching data in batches can improve performance and prevent memory overload.</p> </li> </ul>"},{"location":"ingest-command/ingestJDBC/#example-ingestjdbc-command-with-dynamic-incremental-load","title":"Example IngestJDBC Command with Dynamic Incremental Load","text":"<p>Here\u2019s an example of how the IngestJDBC command might be configured to perform incremental loading with dynamically fetched lower and upper bounds:</p> <pre><code>{\n  \"execution\": [\n    {\n      \"type\": \"IngestJDBC\",\n      \"name\": \"jdbc_connection_customers\",\n      \"connection_name\": \"jdbc_connection\",\n      \"query\": \"SELECT * FROM crm.customers WHERE updated_at BETWEEN '{{lower_bound_value}}' AND '{{upper_bound_value}}'\",\n      \"lower_bound_query\": \"SELECT last_run from stats.incremental where name='jdbc_connection_customers'\",\n      \"upper_bound_query\": \"SELECT MAX(updated_at) FROM crm.customers\",\n      \"incremental_value_storage\": {\n        \"type\": \"table\",\n        \"connection\": \"stats.incremental\",\n      }\n      \"batch_size\": 1000\n    }\n  ]\n}\n</code></pre>"},{"location":"ingest-command/ingestJDBC/#conclusion","title":"Conclusion","text":"<p>The \"IngestJDBC\" command with dynamic lower and upper bound queries enables more flexible and efficient incremental data loading. By querying the database for the appropriate range of data (via lower_bound_query and upper_bound_query), this approach avoids redundant data loads and ensures that only relevant data is processed.</p> <p>This dynamic approach to handling incremental data ingestion is ideal for large databases or when frequent data loads are required, as it reduces the amount of data ingested and improves pipeline performance.</p> <p>By leveraging SQL queries for both incremental bounds and the actual data fetch, you gain fine-grained control over the data extraction process. This configuration is highly customizable, allowing you to handle a variety of data sources and update scenarios.</p>"},{"location":"ingest-command/ingestKafka/","title":"Kafka Batch Ingestion Configuration - Coming Soon","text":""},{"location":"ingest-command/ingestKafka/#kafka-batch-ingestion-command-documentation","title":"Kafka Batch Ingestion Command Documentation \ud83d\udce5","text":"<p>The IngestKafka can be used in batch mode to periodically read messages from a Kafka topic at specified intervals. The configuration includes parameters to handle batch timing, max poll records, and other Kafka consumer settings.</p>"},{"location":"ingest-command/ingestKafka/#json-configuration-example","title":"JSON Configuration Example","text":"<pre><code>{\n  \"execution\": [\n    {\n      \"type\": \"IngestKafka\",\n      \"bootstrap_servers\": \"localhost:9092\",\n      \"topic\": \"example-topic\",\n      \"group_id\": \"batch-consumer-group\",\n      \"key_deserializer\": \"org.apache.kafka.common.serialization.StringDeserializer\",\n      \"value_deserializer\": \"org.apache.kafka.common.serialization.StringDeserializer\",\n      \"auto_offset_reset\": \"earliest\",\n      \"batch_interval\": 60000,  // 60 seconds, adjust as needed\n      \"max_poll_records\": 1000,\n      \"additional_attributes\": [\n        {\n          \"key\": \"enable.auto.commit\",\n          \"value\": \"false\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"ingest-command/ingestKafka/#parameters","title":"Parameters","text":"Parameter Type Required Description <code>type</code> String \u2705 Yes The type of command (<code>IngestKafka</code>). <code>bootstrap_servers</code> String \u2705 Yes The Kafka cluster address (e.g., <code>localhost:9092</code>). <code>topic</code> String \u2705 Yes The Kafka topic from which data will be consumed. <code>group_id</code> String \u2705 Yes The consumer group ID for Kafka. <code>key_deserializer</code> String \u2705 Yes Deserializer for the Kafka message key (e.g., <code>org.apache.kafka.common.serialization.StringDeserializer</code>). <code>value_deserializer</code> String \u2705 Yes Deserializer for the Kafka message value (e.g., <code>org.apache.kafka.common.serialization.StringDeserializer</code>). <code>auto_offset_reset</code> String \u2705 Yes Determines what to do when there is no initial offset or the offset is out of range (<code>earliest</code> or <code>latest</code>). <code>batch_interval</code> Integer \u2705 Yes Defines the batch interval in milliseconds (e.g., <code>60000</code> for 60 seconds). <code>max_poll_records</code> Integer \u2705 Yes Specifies the maximum number of records to fetch in each poll (e.g., <code>1000</code>). <code>additional_attributes</code> Array \u274c No Additional Kafka consumer settings, such as enabling auto-commit or configuring poll records."},{"location":"ingest-command/ingestKafka/#batch-mode-specific-parameters","title":"Batch Mode Specific Parameters","text":"<ul> <li> <p><code>batch_interval</code>: Specifies the frequency (in milliseconds) at which the data will be ingested in batch mode. For example, setting this to <code>60000</code> means Kafka will be polled every 60 seconds for new data.</p> </li> <li> <p><code>max_poll_records</code>: Controls the maximum number of records to fetch in each batch. Adjust this based on your performance needs.</p> </li> </ul>"},{"location":"ingest-command/ingestKafka/#example-use-case","title":"Example Use Case","text":"<p>Scenario: You want to read data from the <code>example-topic</code> Kafka topic in batch mode every 60 seconds, using a consumer group called <code>batch-consumer-group</code>, with a maximum of 1000 records per poll.</p>"},{"location":"ingest-command/ingestKafka/#json-configuration","title":"JSON Configuration:","text":"<pre><code>{\n  \"execution\": [\n    {\n      \"type\": \"KafkaRead\",\n      \"bootstrap_servers\": \"localhost:9092\",\n      \"topic\": \"example-topic\",\n      \"group_id\": \"batch-consumer-group\",\n      \"key_deserializer\": \"org.apache.kafka.common.serialization.StringDeserializer\",\n      \"value_deserializer\": \"org.apache.kafka.common.serialization.StringDeserializer\",\n      \"auto_offset_reset\": \"earliest\",\n      \"batch_interval\": 60000,  // 60 seconds, adjust as needed\n      \"max_poll_records\": 1000,\n      \"additional_attributes\": [\n        {\n          \"key\": \"enable.auto.commit\",\n          \"value\": \"false\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"ingest-command/ingestKafka/#behavior","title":"Behavior","text":"<ul> <li> <p><code>batch_interval</code>: The frequency at which the pipeline reads messages from Kafka. In batch mode, the data is ingested periodically rather than continuously. For example, if set to <code>60000</code>, the pipeline will poll Kafka every 60 seconds for new messages.</p> </li> <li> <p><code>max_poll_records</code>: Controls how many records will be fetched in a single batch. Set this based on the expected load and performance requirements.</p> </li> <li> <p><code>auto_offset_reset</code>: Determines where to start consuming Kafka messages if no offset is committed. <code>earliest</code> means that messages will be consumed from the beginning of the topic, and <code>latest</code> means consumption will begin with the newest messages.</p> </li> <li> <p><code>additional_attributes</code>: Allows additional settings for the Kafka consumer. For example, setting <code>enable.auto.commit</code> to <code>false</code> ensures that offsets are not automatically committed, giving you more control over when offsets are committed.</p> </li> </ul>"},{"location":"ingest-command/ingestKafka/#related-commands","title":"Related Commands","text":""},{"location":"ingest-command/ingestKafka/#conclusion","title":"Conclusion","text":"<p>The IngestKafka in batch mode is a powerful way to periodically ingest data from Kafka topics. By configuring parameters like <code>batch_interval</code>, <code>max_poll_records</code>, and <code>auto_offset_reset</code>, you can control how and when Kafka data is ingested, making it well-suited for periodic or batch-based processing in your data pipeline.</p>"},{"location":"streaming-command/stream-kafka-iceberg/","title":"StreamKafkaIceberg Command Documentation - Coming Soon","text":""},{"location":"streaming-command/stream-kafka-iceberg/#overview","title":"Overview","text":"<p>The StreamKafkaIceberg command facilitates the streaming ingestion of data from Apache Kafka into an Iceberg table. This command is ideal for real-time data pipelines where you want to capture streaming data from Kafka topics and store it in Iceberg tables with ACID compliance and schema evolution capabilities.</p> <p>This command supports consuming data from Kafka topics in real-time and writing it to Iceberg tables, enabling you to leverage Iceberg's powerful features like partitioning, schema evolution, and efficient querying for analytical workloads.</p>"},{"location":"streaming-command/stream-kafka-iceberg/#streamkafkaiceberg-command-structure","title":"StreamKafkaIceberg Command Structure","text":"Parameter Type Required Description <code>type</code> String \u2705 Yes Always set to <code>\"StreamKafkaIceberg\"</code>. <code>kafka_connection</code> String \u2705 Yes The name of the Kafka connection (referenced from Connection Properties). <code>topic</code> String \u2705 Yes The Kafka topic to consume from. <code>iceberg_table</code> String \u2705 Yes The Iceberg table where the data will be written. <code>checkpoint_location</code> String \u2705 Yes The location for storing checkpoint information to track Kafka offsets. <code>group_id</code> String \u2705 Yes The Kafka consumer group ID used to track message consumption. <code>format</code> String \u2705 Yes The format to use for writing to the Iceberg table (e.g., <code>parquet</code>). <code>poll_timeout_ms</code> Integer \u274c No Timeout in milliseconds for polling data from Kafka (default: <code>1000</code>). <code>max_offset</code> Long \u274c No The maximum offset to consume up to from Kafka (default: latest offset)."},{"location":"streaming-command/stream-kafka-iceberg/#detailed-explanation-of-parameters","title":"Detailed Explanation of Parameters","text":"<ul> <li> <p><code>type</code>: Always set to <code>\"StreamKafkaIceberg\"</code>. This identifies the command type as a Kafka streaming ingestion command to an Iceberg table.</p> </li> <li> <p><code>kafka_connection</code>: The name of the Kafka connection to use for consuming messages from Kafka. This connection is defined in the Connection Properties section, which specifies the necessary credentials and configurations to connect to the Kafka broker.</p> </li> <li> <p><code>topic</code>: Specifies the Kafka topic to consume messages from. Kafka topics are logical channels that hold streams of data, and this parameter tells the system from which topic to read data.</p> </li> <li> <p><code>iceberg_table</code>: The name of the Iceberg table to write the streaming data. This is where the data consumed from Kafka will be written. Iceberg tables provide strong consistency, partitioning, and support for schema evolution.</p> </li> <li> <p><code>checkpoint_location</code>: The location to store Kafka offset information, ensuring that the consumer can resume from the last processed message in case of failure or restart. This is crucial for maintaining exactly-once processing semantics.</p> </li> <li> <p><code>group_id</code>: The consumer group ID to track the Kafka consumer\u2019s state. Multiple consumers can share the same group ID to divide the workload, and each group ID will independently keep track of the Kafka offset.</p> </li> <li> <p><code>format</code>: The format for writing data to the Iceberg table. Common formats include <code>parquet</code>, <code>orc</code>, or <code>avro</code>. Parquet is often recommended due to its efficient columnar storage and compatibility with Iceberg.</p> </li> <li> <p><code>poll_timeout_ms</code>: This optional parameter defines the timeout in milliseconds for polling new data from Kafka. The default is typically 1000ms, but you can adjust it based on the frequency of data and system responsiveness.</p> </li> <li> <p><code>max_offset</code>: The maximum Kafka offset to consume messages up to. If not provided, it defaults to consuming from the latest offset, i.e., only new messages that arrive after the job starts.</p> </li> </ul>"},{"location":"streaming-command/stream-kafka-iceberg/#example-streamkafkaiceberg-command","title":"Example StreamKafkaIceberg Command","text":"<p>Below is an example configuration for streaming data from a Kafka topic into an Iceberg table:</p> <p>```json {   \"execution\": [     {       \"type\": \"StreamKafkaIceberg\",       \"kafka_connection\": \"kafka_connection\",       \"topic\": \"customer_events\",       \"iceberg_table\": \"crm.raw.customer_events\",       \"checkpoint_location\": \"/path/to/checkpoints\",       \"group_id\": \"customer_events_consumer_group\",       \"format\": \"parquet\",       \"poll_timeout_ms\": 2000,       \"max_offset\": 1000000     }   ] }</p>"},{"location":"streaming-command/stream-kafka-iceberg/#conclusion","title":"Conclusion","text":"<p>The StreamKafkaIceberg command is an essential tool for real-time data ingestion in a data pipeline. It leverages Apache Kafka as a source for streaming data and Iceberg as the destination for structured and high-performance storage. This combination of streaming with Kafka and efficient data management with Iceberg enables the creation of scalable, reliable, and performant data pipelines.</p> <p>With the ability to dynamically consume data from Kafka topics and write to Iceberg tables, you get the benefit of real-time analytics, partitioned storage, schema evolution, and ACID compliance.</p> <p>By configuring checkpointing and offset management, this command supports fault tolerance and exactly-once processing semantics, ensuring your data pipeline is robust and capable of handling large volumes of real-time data.</p>"},{"location":"transform-command/transformSQL/","title":"TransformSQL Command Documentation - Coming Soon","text":""},{"location":"transform-command/transformSQL/#transformsql-command-configuration-example","title":"TransformSQL Command Configuration Example","text":"<p>The TransformSQL command allows you to apply SQL transformations to your data before it is written to the destination. It is useful for performing data processing, filtering, aggregations, and other SQL-based operations within your data pipeline.</p> <pre><code>{\n  \"execution\": [\n    {\n      \"type\": \"TransformSQL\",\n      \"query\": \"SELECT * FROM raw_data WHERE age &gt; 18\"\n    }\n  ]\n}\n\n</code></pre>"},{"location":"transform-command/transformSQL/#detailed-explanation-of-parameters","title":"Detailed Explanation of Parameters","text":"<ul> <li> <p><code>type</code>: Always set to <code>\"TransformSQL\"</code>. This parameter indicates that the command is executing a SQL transformation. It is used to identify the command type within the pipeline configuration.</p> </li> <li> <p><code>query</code>: The SQL query that will be executed on the source data. This query can contain all valid SQL operations such as <code>SELECT</code>, <code>WHERE</code>, <code>JOIN</code>, <code>GROUP BY</code>, and other SQL constructs. It is the core of the transformation and allows you to filter, aggregate, or manipulate the data before it is written to the destination. It can also be a file and provide file for sql statement</p> </li> </ul>"},{"location":"transform-command/transformSQL/#conclusion","title":"Conclusion","text":"<p>The TransformSQL command is a powerful tool that enables users to perform SQL-based transformations on data within a pipeline. It provides an easy-to-use interface for applying filters, aggregations, and other transformations using SQL queries, and the results can be stored in a designated destination. This allows data engineers to manage and manipulate data efficiently before it reaches its final destination.</p> <p>The flexibility of SQL combined with the power of the pipeline framework makes TransformSQL an essential component for complex data workflows. By leveraging SQL, users can write complex transformations in a familiar and scalable manner, ensuring data quality and enabling the right processing logic for their applications.</p> <p>Whether it's filtering records, joining tables, or performing complex aggregations, the TransformSQL command simplifies the integration of SQL-based logic within the data pipeline, making it an indispensable tool for data transformation.</p>"}]}