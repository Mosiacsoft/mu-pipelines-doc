{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Mu-Pipelines Documentation \ud83d\ude80","text":"<p>Welcome to the Mu-Pipelines documentation! Mu-Pipelines is a configuration-driven data pipeline platform that enables easy data ingestion, transformation, and orchestration without complex coding.</p>"},{"location":"#features","title":"\ud83c\udf1f Features","text":"<ul> <li>Flexible Data Ingestion: Read from CSV, JSON, Parquet, Databases, and more.</li> <li>Powerful Transformations: SQL and Python-based processing.</li> <li>Seamless Data Integration: Supports Iceberg, Delta Lake, and more.</li> <li>Lightweight &amp; Scalable: Configuration-based, no heavy setup required.</li> </ul> <p>\ud83d\udcd6 Start by reading the Getting Started guide!</p>"},{"location":"connection_properties/","title":"Connection properties","text":"<p>comming soon </p>"},{"location":"faq/","title":"FAQ \u2753","text":""},{"location":"faq/#how-do-i-install-mu-pipelines","title":"How do I install Mu-Pipelines?","text":"<p>Run: ```sh pip install mu-pipelines</p>"},{"location":"faq/#can-i-build-my-own-connector","title":"Can i build my own connector","text":"<p>Yes, that functionality is available in pro license</p>"},{"location":"faq/#how-do-i-add-connectionspasswords-to-my-source-and-destination","title":"How do i add connections/passwords to my source and destination","text":"<p>You can add them in connection_properties.json file in root folder. For details see connection_properties page </p>"},{"location":"faq/#what-is-the-role-for-global_configjson","title":"What is the role for global_config.json","text":"<p>There are ceratin global properties you can apply to your whole project. See global_properties.json page for detailed analysis </p>"},{"location":"faq/#what-is-the-role-of-library","title":"What is the role of library","text":"<p>In future we plan to support core python/pandas in addition to spark. As we release library can be switched</p>"},{"location":"getting-started/","title":"Getting Started \ud83d\ude80","text":"<p>Welcome! This guide helps you set up and run Mu-Pipelines.</p>"},{"location":"getting-started/#1-install-mu-pipelines","title":"1\ufe0f Install Mu-Pipelines","text":"<p>Ensure you have Python 3 installed, then run: ```sh pip install mu-pipelines</p>"},{"location":"getting-started/#2-create-config-files","title":"2 Create Config files","text":"<p>Our config files are divided in two section execute and destination </p> <p>Configuration Parameters: execute-xx: This can be ingesting data from source or transforming using sql/python. destination: The destination where data should be stored.</p> <p>Refer config documentation for list of execute and destination command available </p>"},{"location":"getting-started/#3-run-a-sample-config-file","title":"3 Run a sample config file","text":"<p>In your notebook/py file add below code </p> <p>from mu_pipelines_driver.run_config import run_config df = run_config(     [         {             \"execution\": [                 {                     \"type\": \"CSVReadCommand\",                     \"file_location\": \"/home/iceberg/data/file/people.csv\",                     \"delimiter\": \",\",                     \"quotes\": \"\\\"\",                     \"additional_attributes\": [                         { \"key\": \"header\", \"value\": \"True\" }                     ]                 }             ],             \"destination\": [                 {                     \"type\": \"table\",                     \"table_name\": \"crm.raw.people\",                     \"mode\": \"overwrite\"                 }             ]         }     ],     {         \"library\": \"spark\"     },     {         \"connections\":[]     } )</p>"},{"location":"getting-started/#4-run-config-using-spark-submit","title":"4 Run config using spark submit","text":""},{"location":"getting-started/#run-config-using-airflow","title":"Run config using Airflow","text":""},{"location":"getting-started/#5-chain-different-configs-to-meet-business-needs","title":"5 Chain different config's to meet business needs","text":"<p>\ud83c\udfaf Want to add a custom connector? Stay tuned for developer guides!</p>"},{"location":"global_properties/","title":"Global properties","text":"<p>Details coming soon !!</p>"},{"location":"destination-command/iceberg/","title":"Iceberg","text":""},{"location":"destination-command/iceberg/#iceberg-as-destination-documentation","title":"Iceberg as Destination Documentation \ud83d\udcca","text":"<p>The Iceberg destination configuration allows you to define where the data will be written in the pipeline. The destination can specify an Iceberg table where data will be loaded, and it supports different modes for managing data.</p>"},{"location":"destination-command/iceberg/#json-configuration-example","title":"JSON Configuration Example","text":"<pre><code>{\n  \"destination\": [\n    {\n      \"type\": \"table\",\n      \"table_name\": \"crm.raw.people\",\n      \"mode\": \"overwrite\"\n    }\n  ]\n}\n</code></pre>"},{"location":"destination-command/iceberg/#parameters","title":"Parameters","text":"Parameter Type Required Description <code>type</code> String \u2705 Yes The destination type (<code>table</code> for Iceberg tables). <code>table_name</code> String \u2705 Yes The name of the Iceberg table where data will be written. <code>mode</code> String \u2705 Yes Defines how the data is written to the table. Possible values: <code>overwrite</code>, <code>append</code>, etc."},{"location":"destination-command/iceberg/#mode-options","title":"Mode Options","text":"<ul> <li><code>overwrite</code>: Replaces the data in the specified table with the new data. Any existing records in the table will be deleted.</li> <li><code>append</code>: Adds the new data to the existing records in the table without modifying the existing data.</li> </ul>"},{"location":"destination-command/iceberg/#configuring-spark-with-iceberg-as-the-default-catalog","title":"Configuring Spark with Iceberg as the Default Catalog","text":"<p>To ensure that Apache Spark can interact with Iceberg as the default catalog when using it as a destination, you must configure Spark with the appropriate catalog settings. By default, Spark uses Hive as its catalog. To set Iceberg as the default, follow these steps:</p> <ol> <li>Add Iceberg as a dependency:    You need to add the Iceberg Spark extension to your Spark setup. Ensure the correct version of Iceberg is available in your Spark environment. Add the following dependency to your Spark session configuration:</li> </ol> <p><code>bash    --packages org.apache.iceberg:iceberg-spark3-runtime:0.13.1</code></p> <p>This will include the necessary Iceberg libraries for Spark.</p> <ol> <li>Configure the Default Catalog:    To set Iceberg as the default catalog in Spark, update the Spark session configuration. You can set Iceberg as the default catalog by using the following settings:</li> </ol> <p><code>bash    spark.sql.catalog.spark_catalog org.apache.iceberg.spark.SparkSessionCatalog    spark.sql.catalog.spark_catalog.type hadoop    spark.sql.catalog.spark_catalog.warehouse /path/to/warehouse</code></p> <ul> <li><code>spark_catalog</code> refers to the catalog in use (in this case, Iceberg).</li> <li>The <code>warehouse</code> location should point to the directory where your Iceberg tables are stored.</li> </ul> <p>With this configuration, Spark will use Iceberg as the default catalog for reading and writing tables.</p> <ol> <li>Setting Iceberg Table for the Destination:    Once Iceberg is set as the default catalog, you can specify the destination table in your pipeline configuration, like so:</li> </ol> <p><code>json    {      \"destination\": [        {          \"type\": \"table\",          \"table_name\": \"crm.raw.people\",          \"mode\": \"overwrite\"        }      ]    }</code></p> <p>Here, <code>crm.raw.people</code> refers to an Iceberg table that will be written to, and Spark will handle the interaction with the Iceberg catalog.</p>"},{"location":"destination-command/iceberg/#example-use-case","title":"Example Use Case","text":"<p>Scenario: You want to write data to the <code>crm.raw.people</code> Iceberg table and overwrite any existing records.</p>"},{"location":"destination-command/iceberg/#json-configuration","title":"JSON Configuration:","text":"<pre><code>{\n  \"destination\": [\n    {\n      \"type\": \"table\",\n      \"table_name\": \"crm.raw.people\",\n      \"mode\": \"overwrite\"\n    }\n  ]\n}\n</code></pre>"},{"location":"destination-command/iceberg/#behavior","title":"Behavior","text":"<ul> <li><code>table_name</code>: Specifies the Iceberg table where the data should be loaded.</li> <li><code>mode</code>: Controls how the data is written to the destination table. Choose <code>overwrite</code> to replace existing data or <code>append</code> to add new records to the existing data.</li> </ul>"},{"location":"destination-command/iceberg/#related-commands","title":"Related Commands","text":"<ul> <li>TransformSQL: For transforming the data before writing it to the destination.</li> </ul>"},{"location":"destination-command/iceberg/#conclusion","title":"Conclusion","text":"<p>The Iceberg as Destination command provides a simple and effective way to write data into Iceberg tables with flexible write modes like <code>overwrite</code> and <code>append</code>. Configuring Spark with Iceberg as the default catalog makes it easier to work with Iceberg tables without needing to manually specify the catalog for each interaction. Once configured, use this command to manage your data pipeline outputs in a distributed, scalable table format.</p>"},{"location":"execute-command/CSVRead/","title":"CSVRead","text":""},{"location":"execute-command/CSVRead/#csvread-documentation","title":"CSVRead Documentation \ud83d\udcc4","text":"<p>The <code>CSVRead</code> is used to read and load data from a CSV file into the pipeline. It provides configuration options like file location, delimiter, quote characters, and headers to accommodate different CSV formats.</p>"},{"location":"execute-command/CSVRead/#json-configuration-example","title":"JSON Configuration Example","text":"<pre><code>{\n  \"execution\": [\n    {\n      \"type\": \"CSVRead\",\n      \"file_location\": \"/home/iceberg/data/file/people.csv\",\n      \"delimiter\": \",\",\n      \"quotes\": \"\"\",\n      \"additional_attributes\": [\n        {\n          \"key\": \"header\",\n          \"value\": \"True\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"execute-command/CSVRead/#parameters","title":"Parameters","text":"Parameter Type Required Description <code>type</code> String \u2705 Yes Specifies the type of the command (<code>CSVReadCommand</code>). <code>file_location</code> String \u2705 Yes The path to the CSV file to be read. Can be absolute or relative. <code>delimiter</code> String \u274c No The character used to separate columns in the CSV (default is <code>,</code>). <code>quotes</code> String \u274c No The character used for quoting text in CSV (default is <code>\"</code>). <code>additional_attributes</code> Array \u274c No Optional attributes to specify additional settings. Each attribute consists of a <code>key</code> and <code>value</code> pair. For example, you can specify if the CSV has headers."},{"location":"execute-command/CSVRead/#additional-attributes-example","title":"Additional Attributes Example","text":"<p>The <code>additional_attributes</code> parameter allows users to specify extra properties for the CSV file:</p> <ul> <li>key: The name of the attribute.</li> <li>value: The value associated with the attribute.</li> </ul>"},{"location":"execute-command/CSVRead/#example-of-header-attribute","title":"Example of Header Attribute","text":"<pre><code>\"additional_attributes\": [\n  {\n    \"key\": \"header\",\n    \"value\": \"True\"\n  }\n]\n</code></pre> <p>This indicates that the first row in the CSV file should be treated as column headers.</p>"},{"location":"execute-command/CSVRead/#example-use-case","title":"Example Use Case","text":"<p>Scenario: You need to read a CSV file named <code>people.csv</code> that has headers and uses commas as delimiters.</p>"},{"location":"execute-command/CSVRead/#json-configuration","title":"JSON Configuration:","text":"<pre><code>{\n  \"execution\": [\n    {\n      \"type\": \"CSVRead\",\n      \"file_location\": \"/home/iceberg/data/file/people.csv\",\n      \"delimiter\": \",\",\n      \"quotes\": \"\"\",\n      \"additional_attributes\": [\n        {\n          \"key\": \"header\",\n          \"value\": \"True\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"execute-command/CSVRead/#behavior","title":"Behavior","text":"<ul> <li><code>file_location</code>: Specifies the file to be read. Ensure the path is correct.</li> <li><code>delimiter</code>: By default, CSV files are comma-delimited, but this can be customized for other formats (e.g., semicolons).</li> <li><code>quotes</code>: Handles quoted values in CSV files (e.g., <code>\"John, Doe\"</code>).</li> <li><code>header</code>: If set to <code>True</code>, the first row is treated as column headers.</li> </ul>"},{"location":"execute-command/CSVRead/#related-commands","title":"Related Commands","text":"<ul> <li>CSVWrite: For writing data back into a CSV file.</li> <li>TransformSQL: For transforming the data after reading it.</li> </ul>"},{"location":"execute-command/CSVRead/#conclusion","title":"Conclusion","text":"<p>The CSVRead is a flexible and configurable way to read CSV files into your pipeline, allowing for custom delimiters, quotes, and handling of headers.</p>"},{"location":"execute-command/kafka/","title":"Kafka Batch Ingestion Configuration","text":""},{"location":"execute-command/kafka/#kafka-batch-ingestion-configuration-example","title":"Kafka Batch Ingestion Configuration Example","text":"<p>To ingest data from a Kafka topic in batch mode, you would typically configure your pipeline to read data from Kafka at fixed intervals, similar to how batch jobs are run. Below is an example configuration for ingesting data in batch mode from Kafka.</p> <pre><code>{\n  \"execution\": [\n    {\n      \"type\": \"KafkaRead\",\n      \"bootstrap_servers\": \"localhost:9092\",\n      \"topic\": \"example-topic\",\n      \"group_id\": \"batch-consumer-group\",\n      \"key_deserializer\": \"org.apache.kafka.common.serialization.StringDeserializer\",\n      \"value_deserializer\": \"org.apache.kafka.common.serialization.StringDeserializer\",\n      \"auto_offset_reset\": \"earliest\",\n      \"batch_interval\": 60000,  // 60 seconds, adjust as needed\n      \"max_poll_records\": 1000,\n      \"additional_attributes\": [\n        {\n          \"key\": \"enable.auto.commit\",\n          \"value\": \"false\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"execute-command/kafka/#kafka-batch-ingestion-command-documentation","title":"Kafka Batch Ingestion Command Documentation \ud83d\udce5","text":"<p>The KafkaReadCommand can be used in batch mode to periodically read messages from a Kafka topic at specified intervals. The configuration includes parameters to handle batch timing, max poll records, and other Kafka consumer settings.</p>"},{"location":"execute-command/kafka/#json-configuration-example","title":"JSON Configuration Example","text":"<pre><code>{\n  \"execution\": [\n    {\n      \"type\": \"KafkaRead\",\n      \"bootstrap_servers\": \"localhost:9092\",\n      \"topic\": \"example-topic\",\n      \"group_id\": \"batch-consumer-group\",\n      \"key_deserializer\": \"org.apache.kafka.common.serialization.StringDeserializer\",\n      \"value_deserializer\": \"org.apache.kafka.common.serialization.StringDeserializer\",\n      \"auto_offset_reset\": \"earliest\",\n      \"batch_interval\": 60000,  // 60 seconds, adjust as needed\n      \"max_poll_records\": 1000,\n      \"additional_attributes\": [\n        {\n          \"key\": \"enable.auto.commit\",\n          \"value\": \"false\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"execute-command/kafka/#parameters","title":"Parameters","text":"Parameter Type Required Description <code>type</code> String \u2705 Yes The type of command (<code>KafkaReadCommand</code>). <code>bootstrap_servers</code> String \u2705 Yes The Kafka cluster address (e.g., <code>localhost:9092</code>). <code>topic</code> String \u2705 Yes The Kafka topic from which data will be consumed. <code>group_id</code> String \u2705 Yes The consumer group ID for Kafka. <code>key_deserializer</code> String \u2705 Yes Deserializer for the Kafka message key (e.g., <code>org.apache.kafka.common.serialization.StringDeserializer</code>). <code>value_deserializer</code> String \u2705 Yes Deserializer for the Kafka message value (e.g., <code>org.apache.kafka.common.serialization.StringDeserializer</code>). <code>auto_offset_reset</code> String \u2705 Yes Determines what to do when there is no initial offset or the offset is out of range (<code>earliest</code> or <code>latest</code>). <code>batch_interval</code> Integer \u2705 Yes Defines the batch interval in milliseconds (e.g., <code>60000</code> for 60 seconds). <code>max_poll_records</code> Integer \u2705 Yes Specifies the maximum number of records to fetch in each poll (e.g., <code>1000</code>). <code>additional_attributes</code> Array \u274c No Additional Kafka consumer settings, such as enabling auto-commit or configuring poll records."},{"location":"execute-command/kafka/#batch-mode-specific-parameters","title":"Batch Mode Specific Parameters","text":"<ul> <li> <p><code>batch_interval</code>: Specifies the frequency (in milliseconds) at which the data will be ingested in batch mode. For example, setting this to <code>60000</code> means Kafka will be polled every 60 seconds for new data.</p> </li> <li> <p><code>max_poll_records</code>: Controls the maximum number of records to fetch in each batch. Adjust this based on your performance needs.</p> </li> </ul>"},{"location":"execute-command/kafka/#example-use-case","title":"Example Use Case","text":"<p>Scenario: You want to read data from the <code>example-topic</code> Kafka topic in batch mode every 60 seconds, using a consumer group called <code>batch-consumer-group</code>, with a maximum of 1000 records per poll.</p>"},{"location":"execute-command/kafka/#json-configuration","title":"JSON Configuration:","text":"<pre><code>{\n  \"execution\": [\n    {\n      \"type\": \"KafkaRead\",\n      \"bootstrap_servers\": \"localhost:9092\",\n      \"topic\": \"example-topic\",\n      \"group_id\": \"batch-consumer-group\",\n      \"key_deserializer\": \"org.apache.kafka.common.serialization.StringDeserializer\",\n      \"value_deserializer\": \"org.apache.kafka.common.serialization.StringDeserializer\",\n      \"auto_offset_reset\": \"earliest\",\n      \"batch_interval\": 60000,  // 60 seconds, adjust as needed\n      \"max_poll_records\": 1000,\n      \"additional_attributes\": [\n        {\n          \"key\": \"enable.auto.commit\",\n          \"value\": \"false\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"execute-command/kafka/#behavior","title":"Behavior","text":"<ul> <li> <p><code>batch_interval</code>: The frequency at which the pipeline reads messages from Kafka. In batch mode, the data is ingested periodically rather than continuously. For example, if set to <code>60000</code>, the pipeline will poll Kafka every 60 seconds for new messages.</p> </li> <li> <p><code>max_poll_records</code>: Controls how many records will be fetched in a single batch. Set this based on the expected load and performance requirements.</p> </li> <li> <p><code>auto_offset_reset</code>: Determines where to start consuming Kafka messages if no offset is committed. <code>earliest</code> means that messages will be consumed from the beginning of the topic, and <code>latest</code> means consumption will begin with the newest messages.</p> </li> <li> <p><code>additional_attributes</code>: Allows additional settings for the Kafka consumer. For example, setting <code>enable.auto.commit</code> to <code>false</code> ensures that offsets are not automatically committed, giving you more control over when offsets are committed.</p> </li> </ul>"},{"location":"execute-command/kafka/#related-commands","title":"Related Commands","text":"<ul> <li>KafkaWriteCommand: Write data to a Kafka topic in batch mode.</li> <li>BatchProcessCommand: For scheduling periodic or batch processing tasks.</li> <li>FileSinkCommand: To write the consumed Kafka data to a file (e.g., CSV, Parquet).</li> <li>TransformCommand: For transforming Kafka data before further processing or storing.</li> </ul>"},{"location":"execute-command/kafka/#conclusion","title":"Conclusion","text":"<p>The KafkaReadCommand in batch mode is a powerful way to periodically ingest data from Kafka topics. By configuring parameters like <code>batch_interval</code>, <code>max_poll_records</code>, and <code>auto_offset_reset</code>, you can control how and when Kafka data is ingested, making it well-suited for periodic or batch-based processing in your data pipeline.</p>"}]}